{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"softmax_regression_minibatch_gd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNCpshjjRlY3O2U4Y+5Sgtk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jTkoTfIbGJna"},"source":["## Softmax regression with mini-batch gradient descent"]},{"cell_type":"markdown","metadata":{"id":"hXt3ZaUwGYVs"},"source":["First, let's import the dependencies. Since we're building everything from scratch, we'll be using numpy and sklearn's `datasets` module for loading the iris dataset."]},{"cell_type":"code","metadata":{"id":"5eV-_OSkjCzg","executionInfo":{"status":"ok","timestamp":1622646095340,"user_tz":-540,"elapsed":506,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["import numpy as np\n","from sklearn import datasets"],"execution_count":465,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98ZgBWqSoUzn","executionInfo":{"status":"ok","timestamp":1622646095710,"user_tz":-540,"elapsed":8,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}},"outputId":"d7677213-7a5a-4aea-c76d-9e4c29432ccc"},"source":["iris = datasets.load_iris()\n","list(iris.keys())"],"execution_count":466,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"]},"metadata":{"tags":[]},"execution_count":466}]},{"cell_type":"markdown","metadata":{"id":"1ISbiuwJGsXm"},"source":["Let's get the independent and depedent data, keeping in mind that we need to initialize a column of ones before any other columns in `X`, for the bias terms.  "]},{"cell_type":"code","metadata":{"id":"y6mbwqd-oWnb","executionInfo":{"status":"ok","timestamp":1622646095711,"user_tz":-540,"elapsed":7,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["X = iris[\"data\"]\n","X_with_bias = np.c_[np.ones((len(X),1)), X]\n","y = iris[\"target\"]"],"execution_count":467,"outputs":[]},{"cell_type":"code","metadata":{"id":"c11PBQKOzsMT","executionInfo":{"status":"ok","timestamp":1622646095711,"user_tz":-540,"elapsed":6,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["num_classes = y.max() + 1"],"execution_count":468,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnXHNwotHClT"},"source":["Next, let's define some useful functions that we'll be using throughout the session:"]},{"cell_type":"code","metadata":{"id":"ZNG0PXUfog34","executionInfo":{"status":"ok","timestamp":1622646095711,"user_tz":-540,"elapsed":5,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["def one_hot_enc(target):\n","  one_hot = np.zeros((target.size, num_classes))\n","  one_hot[np.arange(target.size), target] = 1\n","  return one_hot"],"execution_count":469,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZM7irdYGp0r","executionInfo":{"status":"ok","timestamp":1622646095712,"user_tz":-540,"elapsed":6,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["def softmax(x): \n","  exp = np.exp(x)\n","  return exp / np.expand_dims(np.sum(exp, axis=1), 1)"],"execution_count":470,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0YQmLSuxJne","executionInfo":{"status":"ok","timestamp":1622646095712,"user_tz":-540,"elapsed":5,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["def init_theta(): return np.random.rand(X_with_bias.shape[1], num_classes)"],"execution_count":471,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CY_p2W3H5m5"},"source":["This is a trick that is frequently used in machine learning called `logsumexp`, which is a more efficient way of computing the log of softmax. `epsilon` is a small numerical constant added to ensure that the thing that we are taking the `log` of is not 0 (which would result in `nan`). Read more about `logsumexp` [here](https://blog.feedly.com/tricks-of-the-trade-logsumexp/). There are also numerically stable versions of `logsumexp` but this is not implemented here for simplicity."]},{"cell_type":"code","metadata":{"id":"MFIyISTr2-EW","executionInfo":{"status":"ok","timestamp":1622646096180,"user_tz":-540,"elapsed":4,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["def LogSumExp(y, epsilon):\n","  return y - np.log(np.exp(y).sum(axis = 1, keepdims=True) + epsilon)"],"execution_count":472,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QU12F12ZHRrC"},"source":["Below is a function that generates mini-batches that I've taken from a [stackoverflow](https://stackoverflow.com/questions/38157972/how-to-implement-mini-batch-gradient-descent-in-python) thread."]},{"cell_type":"code","metadata":{"id":"DwtyKdQBe7Cx","executionInfo":{"status":"ok","timestamp":1622646096181,"user_tz":-540,"elapsed":4,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["def iterate_minibatches(x,y,bs,shuffle=True):\n","  assert x.shape[0] == y.shape[0]\n","  if shuffle:\n","    indices = np.arange(x.shape[0])\n","    np.random.shuffle(indices) # shuffles in-place\n","  for start_idx in range(0, x.shape[0], bs):\n","    end_idx = min(start_idx + bs, x.shape[0])\n","    if shuffle:\n","      idx = indices[start_idx: end_idx]\n","    else:\n","      idx = slice(start_idx, end_idx)\n","    yield x[idx], y[idx]"],"execution_count":473,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OqOAz6WhHkdB"},"source":["Since we are not using `sklearn` for anything besides loading the dataset, we also create test, validation and training sets manually. "]},{"cell_type":"code","metadata":{"id":"steD6LpSN39h","executionInfo":{"status":"ok","timestamp":1622646096511,"user_tz":-540,"elapsed":5,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["test_pct = 0.2\n","val_pct = 0.2\n","n_test = int(test_pct * m)\n","n_val = int(val_pct * m)\n","n_train = m - n_test - n_val\n","\n","random_indices = np.random.permutation(m)\n","test_indices = random_indices[:n_test]\n","val_indices = random_indices[n_test:n_test+n_val]\n","train_indices = random_indices[-n_train:]\n","\n","X_train, X_val, X_test = X_with_bias[train_indices], X_with_bias[val_indices], X_with_bias[test_indices]\n","assert X_train.shape[0] == n_train and X_val.shape[0] == n_val and X_test.shape[0] == n_test\n","y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n","assert y_train.shape[0] == n_train and y_val.shape[0] == n_val and y_test.shape[0] == n_test"],"execution_count":474,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzVil3d6Ij9O"},"source":["Now for our main training loop:\n","\n","First, we declare some hyperparameters such as the learning rate (`lr`), number of epochs, batch size (`bs`), and `alpha`, which is the l2 regularisation constant.\n","\n","For each epoch, we iterate through all our mini-batches, get the prediction, and use the prediction and target values to compute the gradient, remembering that we add an additional l2 regularisation term at the end.\n","\n","We also calculate the loss on the validation set to evaluate performance and print it out every 500 epochs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zthiUXoh2kJe","executionInfo":{"status":"ok","timestamp":1622646110675,"user_tz":-540,"elapsed":2900,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}},"outputId":"7a7f092a-979d-4ca3-86a1-26f358d92ff7"},"source":["#Hyperparams\n","lr = 0.1\n","n_epochs = 5001\n","m = X.shape[0]\n","bs = 10 # batch size, so for 150 samples, we need to divide it into 15 groups of 10 instances each\n","n_batches = m//bs\n","epsilon = 1e-7 # to avoid nan results from log(0)\n","alpha = 0.01 # l2 regularisation \n","min_loss = np.inf\n","\n","theta = init_theta()\n","\n","for epoch in range(n_epochs):\n","  for batch in iterate_minibatches(X_train, one_hot_enc(y_train), bs):\n","    xb_train, yb_train = batch\n","    preds = softmax(xb_train@theta)\n","    gradients = (1/xb_train.shape[0]) * (xb_train.T@(preds-yb_train)) + np.vstack([np.zeros((1,num_classes)), alpha * theta[1:]])\n","    theta = theta - lr * gradients\n","\n","  y_val_pred = LogSumExp(X_val@theta, epsilon) \n","  cross_entropy_loss = -np.mean(y_val_pred[np.arange(X_val.shape[0]), y_val]) \n","  l2_loss = alpha * 1/2 * np.square(theta[1:]).sum()\n","  total_loss = cross_entropy_loss + l2_loss\n","\n","  if epoch % 500 == 0: \n","    print(f\"Current loss: {total_loss}\")\n"," "],"execution_count":483,"outputs":[{"output_type":"stream","text":["Current loss: 0.9559835133592577\n","Current loss: 0.2835619290359492\n","Current loss: 0.21752716209131\n","Current loss: 0.23637013398738982\n","Current loss: 0.21555417989668607\n","Current loss: 0.2069807318737825\n","Current loss: 0.20279665261068736\n","Current loss: 0.20305770855358207\n","Current loss: 0.23359282432556988\n","Current loss: 0.2317153779434589\n","Current loss: 0.21075293023173117\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"de4k125KJT11"},"source":["Now, let's see what the final model parameters that we've trained are: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gk5Aduh6JYiU","executionInfo":{"status":"ok","timestamp":1622646110675,"user_tz":-540,"elapsed":20,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}},"outputId":"d8ee473a-d83e-4b05-e7fa-c373878fe7b0"},"source":["theta"],"execution_count":484,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 8.44033927,  2.80588604, -9.26926123],\n","       [-0.16791993,  0.45902131, -0.29110137],\n","       [ 0.99125651, -0.16769693, -0.82355958],\n","       [-2.36250324, -0.15828285,  2.52078609],\n","       [-1.02448477, -0.99657442,  2.02105919]])"]},"metadata":{"tags":[]},"execution_count":484}]},{"cell_type":"markdown","metadata":{"id":"AI52vW7iJZgN"},"source":["And finally, let's use these values to run predictions on the test set and evaluate the performance. "]},{"cell_type":"code","metadata":{"id":"H2q3nUVTEswk","executionInfo":{"status":"ok","timestamp":1622646197196,"user_tz":-540,"elapsed":235,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}}},"source":["preds = np.argmax(X_test@theta, axis=1)"],"execution_count":487,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bYeDM3TSsZY","executionInfo":{"status":"ok","timestamp":1622646197813,"user_tz":-540,"elapsed":8,"user":{"displayName":"Yong Hoon Shin","photoUrl":"","userId":"00286622173541915419"}},"outputId":"92369735-c0a1-4ff1-b058-8b72793189f6"},"source":["np.mean(preds == y_test)"],"execution_count":488,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":488}]},{"cell_type":"markdown","metadata":{"id":"y2G0aseUJuuB"},"source":["100% accuracy ain't bad! That said, this value changes from run to run and may be different when you run the notebook yourself. On my end, however, we seem to be getting at least 95% accuracy consistently. "]}]}